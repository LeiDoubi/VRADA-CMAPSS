from torch.utils.data import Dataset
import numpy as np
import torch


class AbstractCmapssDataset(Dataset):
    """ Abstract dataset class of the CMAPSS,
    This class consists of multiple common functions to preprocessing data, e.g data normalization,
    features extratation.

    """

    def __init__(self,
                 data_path,
                 features_indices=None,
                 normalization=True):
        """ Data set constructor

        Args:
            data_path (str): file path of the data. the file should have the
            extension '.txt.
            features_indices (list or tuple, optional): the indices of the
                features to be extracted. Defaults to None indicating all
                features are retained.

            normalization(bool): normalize the data or not. Default to True.
        """
        super().__init__()
        self.data = np.loadtxt(data_path)
        self.tags = None  # generated by inherited class
        if features_indices is not None:  # extract features
            self._extractFeatures(features_indices)
        if normalization is True:
            self._rescaleData()

    def _extractFeatures(self, features_indices):
        """normalize the given data

        Args:
            features_indices (list or tuple): the indices of the features
            to be extracted.
        """
        # extract features and retain unitID and time step index.
        self.data = self.data[:, [0, 1]+features_indices]

    def _rescaleData(self):
        """rescale the range of each feature to [-1,1]
        """
        # calculate the maxmum and minimum for each feature
        featuresMax = np.max(self.data, axis=0).flatten()[2:]
        featuresMin = np.min(self.data, axis=0).flatten()[2:]
        # map the range of each feature to [-1,1]
        self.data[:, 2:] = 2*(self.data[:, 2:]-featuresMin) / \
            (featuresMax-featuresMin)-1

    def __len__(self):
        return len(self.tags)

    def __getitem__(self, idx):
        if isinstance(self.data, np.ndarray):
            sample = {'seq': torch.from_numpy(self.data[idx, :, :]),
                      'ruls': torch.from_numpy(self.tags[idx, :, :])}
        if isinstance(self.data, list):
            sample = {'seq': torch.from_numpy(self.data[idx]),
                      'ruls': torch.from_numpy(self.tags[idx, :, :])}
        return sample


class CMAPSSTrainingDataset(AbstractCmapssDataset):
    """Training dataset of CMAPSS, which consists of time series data from
    24 sensors. This class consists of multiple functions for data
    preprocessing, e.g data normalization, features extratation,
    time step labling.
    """

    def __init__(self,
                 data_path,
                 max_rul=125,
                 features_indices=None,
                 normalization=True):
        """ Data set constructor

        Args:
            data_path (str): file path of the data. the file should have the
            extension '.txt.
            max_rul (int, optional): the maximum remaining useful life.
                Defaults to 125.
            features_indices (list or tuple, optional): the indices of the
                features to be extracted. Defaults to None indicating all
                features are retained.

            normalization(bool): normalize the data or not. Default to True.
        """
        super().__init__(data_path,
                         features_indices=features_indices,
                         normalization=normalization)
        self.max_rul = max_rul
        # reshape self.data
        timesteps_counts = self._reshape()
        self.tags = self._generateTags(timesteps_counts)

    def _reshape(self):
        """reshape self.data to the form (N_units,Max_len_seq,N_features)

        In oder to accelerate the training process, Mini_batch would be
        utilized in which the length of each sequence should be fixed.
        For those sequences whose length is less than such a fixed value.
        post-0-padding would be applied.

        Returns:
            timesteps_counts(1d numpy.ndarray), count of times steps for every
            unit.

        """
        # make sure data is not reshaped yet
        assert self.data.ndim == 2
        # save old data
        data_old = self.data
        _, indices_first_occur, timesteps_counts = np.unique(
            self.data[:, 0], return_index=True, return_counts=True)
        # calculate start and end index of each unit based on the fact
        # the original data is sorted by unit-id and time circles.
        start_indices = indices_first_occur.flatten()
        # end_index is not included in the range
        end_indices = np.append(start_indices[1:], data_old.shape[0])

        # calculate  maximum length of sequence in the dataset.
        max_length = np.max(timesteps_counts)
        n_units = len(timesteps_counts)
        self.data = np.zeros((n_units, max_length, data_old.shape[1]-2))
        for i, (start_index, end_index)\
                in enumerate(zip(start_indices, end_indices)):
            self.data[i, :end_index-start_index,
                      :] = data_old[start_index:end_index, 2:]
        # for generate tags later
        return timesteps_counts

    def _generateTags(self, timesteps_counts):
        """generate tags for the data set.

        For each unit in the training dataset, the
            last time circle is consider as the totally degraded state and
            Rul is equal to 0. In order to label other data points for the same
            unit, a piecewise linear degration model proposed in
            https://ieeexplore.ieee.org/document/4711422/ would be used here.
            In other words, The Rul is initially equal to (time steps
            is beyond max_rul) or close(time steps less than max_rul) to
            a saturated constant value (here corresponds to max_rul) for each
            engine unit. As the time progress, the Rul will keep decreasing
            linearly until the last time step.

        Args:
            timesteps_counts(1d numpy.ndarray) count of time steps for every
            unit ignoring padding 0 data points.

        """
        # make sure data is reshaped
        assert self.data.ndim == 3
        tags = np.zeros((self.data.shape[0], self.data.shape[1], 1))
        for i in np.arange(self.data.shape[0]):
            if timesteps_counts[i] > self.max_rul+1:
                tags[i, :timesteps_counts[i]-self.max_rul-1, :] = self.max_rul
                tags[i, (timesteps_counts[i] -
                     self.max_rul-1):(timesteps_counts[i]),0] = np.arange(self.max_rul+1)[::-1]
            else:
                tags[i, :timesteps_counts[i], :] = np.arange(
                    timesteps_counts[i])[::-1]
        return tags


class CMAPSSTestDataset(AbstractCmapssDataset):
    """Test dataset of CMAPSS, which consists of time series data from
    24 sensors. This class consists of multiple functions for data
    preprocessing, e.g data normalization, features extratation,
    time step labeling.
    """

    def __init__(self,
                 data_path,
                 tags_path,
                 max_rul=125,
                 features_indices=None,
                 normalization=True):
        """ Data set constructor

        Args:
            data_path (str): file path of the data. the file should have the
            extension '.txt.
            tags_path (str): txt file path of the tags.
            max_rul (int, optional): the maximum remaining useful life.
                Defaults to 125.
            features_indices (list or tuple, optional): the indices of the
                features to be extracted. Defaults to None indicating all
                features are retained.

            normalization(bool): normalize the data or not. Default to True.
        """
        super().__init__(data_path,
                         features_indices=None,
                         normalization=True)
        self.max_rul = max_rul
        # reshape self.data
        self._reshape()
        self.tags = self._generateTags()

    def _reshape(self):
        """ for the evaluation of the module, self.data would be convert to a list
        of numpy.ndarray, with variable length.
        """

        # make sure data is not reshaped yet
        assert isinstance(self.data, np.ndarray) and self.data.ndim == 2
        # save old data
        data_old = self.data
        _, indices_first_occur, timesteps_counts = np.max(
            np.unique(self.data[:, 0], return_index=True, return_counts=True))
        self.data = []
        indices = list(indices_first_occur.flatten())+[data_old.shape[0]]
        for unitId in range(len(indices_first_occur)):
            self.data.append(data_old[indices[unitId]:indices[unitId+1], 2:])

    def _generateTags(self, tags_path):
        """generate tags for the data set.
        For test dataset only the data point in the last time step for
            each engine unit is useful and there is no need to infer the labels
            for other datapoints .

        Args:
            tags_path (str): txt file path of the tags.
        """
        tags = np.loadtxt(tags_path)
        return tags
